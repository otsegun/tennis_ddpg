{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c3cac2",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d56b8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88537709",
   "metadata": {},
   "source": [
    "This project deals with building a reinforcement learning agent to solve the Tennis environment of UnityML.\n",
    "\n",
    "The environment consists a double-jointed arm that can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The state space is an array of 33 elements. It contains the agent's arm position, rotation, velocity, and angular velocities, among others. Given this information, the agent has to learn how to best select actions.  The action space is an arry of 4 numbers containing torque applied to the two joints of the agent's arm.\n",
    "\n",
    "\n",
    "The task is episodic, and in order to solve the environment, **the agent must get an average score of +30 over 100 consecutive episodes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b617b2",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The algorithm used is a DDPG agent consisting of an Actor and a Critic. Both Actor and Critic consist of two neural networks each, the target and the local networks. The architecture of the two Critic neural networks is **state_space_size + action_size x 128 neurons x 128 neurons x 1**. This means that the actions vector and state vector were concatenated and fed into the input layer of the Critic neural networks. The architecture of the two Actors is **state_space_size x 128 neurons x 128 neurons x action_size**.  The hidden layers of the Actor and Critic networks used the **relu activation function**. The final output layer of the two Actor networks used a **tanh activation function**. This helps to clip values of the predicted action vector within the required interval $[-1, 1]$. The final output layer of the two Critic networks did not use any activation function. \n",
    "\n",
    "Since the task is episodic, a maximum of **1000 episodes** was set in order to train the agent. The maximum time step in each episode is set at **1000** with the agent learning every time step (while saving as expericience to a replay buffer, the states, action and rewards obtained in between time steps). The replay buffer size from which data for training is sampled is set at **5e5**. Other parameters used include a **batchsize of 256** used for sampling from the experience buffer, a discount factor of **0.99** and a learning rate of **1e-4** for updating the parameters of both Actor and Critic local networks. Finally an interpolation parameter of $\\tau = 1e-3$ was used in soft updating the parameters of the target neural network. Noise was added to the predicted actions using the Ornstein-Uhlenbeck process with $\\mu = 0$, $\\theta = .15$ and $\\sigma = .1$.\n",
    "\n",
    "The implementation of the agent is an Agent class implemented in the *reacher_agent.py* file. This class initializes the four neural networks used in the algorithm and the replay buffer class (also contained in *reacher_agent.py*) used for storing the experiences that will be sampled and learned from. In addition, the Agent class defines various function for the agent. These include: the **step** function, which steps over each time step and either learns or stores the experience from that time step; the **act** function which chooses an action; the **learn** function which learns from the experience sampled from the replay buffer; and finally the **soft_update** function which updates the target neural network. The neural network architecture used is implemented using pytorch and contained in the *actor_critic_model.py* file. \n",
    "\n",
    "The Agent class is then used in the *Continuous_Control.ipynb* notebook for training the agent in the **ddpg()** function (after initializing the Reacher single agent version linux environment). The parameters of the trained local neural network of the agent are stored in the *checkpoint_actor.pth* and *checkpoint_critic.pth* and they can be reloaded at a later time for use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fa77e",
   "metadata": {},
   "source": [
    "## Results Obtained\n",
    "The environment is considered solved when the agent obtains an average score of at least 30 over the last 100 consecutive episodes. The implementation contained in this projects managed to solve the task in just **380 episodes**, obtaining an average of **30.26** over the previous 100 consecutive episodes. An evolution of the rewards obtained over the episodes while training is shown below: \n",
    "\n",
    "![Rewards](scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf933a42",
   "metadata": {},
   "source": [
    "## Future Improvement\n",
    "An obvious improvement I would like to try is to use the multiple agent environment to solve this task. This should speed up learning. I also want to try prioritized experience replay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd2",
   "language": "python",
   "name": "drlnd2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
