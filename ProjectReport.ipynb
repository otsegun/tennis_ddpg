{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c3cac2",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d56b8",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88537709",
   "metadata": {},
   "source": [
    "This project deals with building a reinforcement learning agent to solve the Tennis environment of UnityML.\n",
    "\n",
    "The environment consists of two agents controlling their rackets to bounce a ball over the net. A reward of +0.1 is given to an agent if it bounces the ball over the net. An agent gets a reward of -o.01 if it plays the ball out of bounds or allows it to hit the gound. Thus, the goal of the agent is to is keep the ball in play for as long as possible.\n",
    "\n",
    "The state space is an array of 8 elements. It contains the position and velocity of the ball and the racket. Given this information, the agent has to learn how to best select actions. The action is an array of two elements corresponding to movement toward/away from the ball and jump. \n",
    "\n",
    "The task is episodic, and in order to solve the environment, **agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b617b2",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The algorithm used is a DDPG agent consisting of an Actor and a Critic. Both Actor and Critic consist of two neural networks each, the target and the local networks. The architecture of the two Critic neural networks is **state_space_size x (128 + action_size) neurons x 128 neurons x 1**. This means that the state vector was fed in at the input layer while the actions were fed in at the first hidden layer of the Critic neural networks. The architecture of the two Actors is **state_space_size x 128 neurons x 128 neurons x action_size**.  The hidden layers of the Actor and Critic networks used the **relu activation function**. The final output layer of the two Actor networks used a **tanh activation function**. The final output layer of the two Critic networks did not use any activation function. \n",
    "\n",
    "Since the task is episodic, a maximum of **1000 episodes** was set in order to train the agent. The agents learn every time step (while saving as expericience to a replay buffer, the states, action and rewards obtained in between time steps). Each episode went on until the end of the episode (no maximum timestep). The replay buffer size from which data for training is sampled is set at **1e6**. Other parameters used include a **batchsize of 256** used for sampling from the experience buffer, a discount factor of **0.99** and a learning rate of **2e-4** for updating the parameters of both Actor and Critic local networks. Finally an interpolation parameter of $\\tau = 1e-3$ was used in soft updating the parameters of the target neural network. Noise was added to the predicted actions using the Ornstein-Uhlenbeck process with $\\mu = 0$, $\\theta = .15$ and $\\sigma = .1$.\n",
    "\n",
    "The implementation of the agent is an Agent class implemented in the *tennis_agent.py* file. This class initializes the four neural networks used in the algorithm and the replay buffer class (also contained in *tennis_agent.py*) used for storing the experiences that will be sampled and learned from. In addition, the Agent class defines various function for the agent. These include: the **step** function, which steps over each time step and either learns or stores the experience from that time step; the **act** function which chooses an action; the **learn** function which learns from the experience sampled from the replay buffer; and finally the **soft_update** function which updates the target neural network. The neural network architecture used is implemented using pytorch and contained in the *actor_critic_model.py* file. \n",
    "\n",
    "The Agent class is then used in the *Tennis.ipynb* notebook for training the agent in the **ddpg()** function (after initializing the  Tennis linux environment). The parameters of the trained local neural network of the agent are stored in the *checkpoint_actor.pth* and *checkpoint_critic.pth* and they can be reloaded at a later time for use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fa77e",
   "metadata": {},
   "source": [
    "## Results Obtained\n",
    "The environment is considered solved when the agent obtains an average score of at least +0.5 over the last 100 consecutive episodes. The implementation contained in this projects managed to solve the task in **826 episodes**, obtaining an average of **+0.5** over the previous 100 consecutive episodes. An evolution of the rewards obtained over the episodes while training is shown below: \n",
    "\n",
    "![Rewards](scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf933a42",
   "metadata": {},
   "source": [
    "## Future Improvement\n",
    "An obvious improvement is to try prioritized experience replay or try to solve the environment using another algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd2",
   "language": "python",
   "name": "drlnd2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
